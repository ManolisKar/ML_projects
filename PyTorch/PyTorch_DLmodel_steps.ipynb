{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Learning Model steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Data\n",
    "\n",
    "The first step is to load and prepare your data.\n",
    "\n",
    "Neural network models require numerical input data and numerical output data.\n",
    "\n",
    "You can use standard Python libraries to load and prepare tabular data, like CSV files. For example, Pandas can be used to load your CSV file, and tools from scikit-learn can be used to encode categorical data, such as class labels.\n",
    "\n",
    "PyTorch provides the Dataset class that you can extend and customize to load your dataset.\n",
    "\n",
    "For example, the constructor of your dataset object can load your data file (e.g. a CSV file). You can then override the __len__() function that can be used to get the length of the dataset (number of rows or samples), and the __getitem__() function that is used to get a specific sample by index.\n",
    "\n",
    "When loading your dataset, you can also perform any required transforms, such as scaling or encoding.\n",
    "\n",
    "A skeleton of a custom Dataset class is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class CSVDataset(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # store the inputs and outputs\n",
    "        self.X = ...\n",
    "        self.y = ...\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, PyTorch provides the DataLoader class to navigate a Dataset instance during the training and evaluation of your model.\n",
    "\n",
    "A DataLoader instance can be created for the training dataset, test dataset, and even a validation dataset.\n",
    "\n",
    "The random_split() function can be used to split a dataset into train and test sets. Once split, a selection of rows from the Dataset can be provided to a DataLoader, along with the batch size and whether the data should be shuffled every epoch.\n",
    "\n",
    "For example, we can define a DataLoader by passing in a selected sample of rows in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "dataset = CSVDataset(...)\n",
    "# select rows from the dataset\n",
    "train, test = random_split(dataset, [[...], [...]])\n",
    "# create a data loader for train and test sets\n",
    "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined, a DataLoader can be enumerated, yielding one batch worth of samples each iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# train the model\n",
    "for i, (inputs, targets) in enumerate(train_dl):\n",
    "\t..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n",
    "\n",
    "The next step is to define a model.\n",
    "\n",
    "The idiom for defining a model in PyTorch involves defining a class that extends the Module class.\n",
    "\n",
    "The constructor of your class defines the layers of the model and the forward() function is the override that defines how to forward propagate input through the defined layers of the model.\n",
    "\n",
    "Many layers are available, such as Linear for fully connected layers, Conv2d for convolutional layers, and MaxPool2d for pooling layers.\n",
    "\n",
    "Activation functions can also be defined as layers, such as ReLU, Softmax, and Sigmoid.\n",
    "\n",
    "Below is an example of a simple MLP model with one layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = Linear(n_inputs, 1)\n",
    "        self.activation = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of a given layer can also be initialized after the layer is defined in the constructor.\n",
    "\n",
    "Common examples include the Xavier and He weight initialization schemes. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "xavier_uniform_(self.layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model\n",
    "\n",
    "The training process requires that you define a loss function and an optimization algorithm.\n",
    "\n",
    "Common loss functions include the following:\n",
    "\n",
    "BCELoss: Binary cross-entropy loss for binary classification.\n",
    "CrossEntropyLoss: Categorical cross-entropy loss for multi-class classification.\n",
    "MSELoss: Mean squared loss for regression.\n",
    "For more on loss functions generally, see the tutorial:\n",
    "\n",
    "[Loss and Loss Functions for Training Deep Learning Neural Networks](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
    "\n",
    "Stochastic gradient descent is used for optimization, and the standard algorithm is provided by the SGD class, although other versions of the algorithm are available, such as Adam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model involves enumerating the DataLoader for the training dataset.\n",
    "\n",
    "First, a loop is required for the number of training epochs. Then an inner loop is required for the mini-batches for stochastic gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# enumerate epochs\n",
    "for epoch in range(100):\n",
    "    # enumerate mini batches\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "    \t..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each update to the model involves the same general pattern comprised of:\n",
    "\n",
    "* Clearing the last error gradient.\n",
    "* A forward pass of the input through the model.\n",
    "* Calculating the loss for the model output.\n",
    "* Backpropagating the error through the model.\n",
    "* Update the model in an effort to reduce loss.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# clear the gradients\n",
    "optimizer.zero_grad()\n",
    "# compute the model output\n",
    "yhat = model(inputs)\n",
    "# calculate loss\n",
    "loss = criterion(yhat, targets)\n",
    "# credit assignment\n",
    "loss.backward()\n",
    "# update model weights\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the model\n",
    "\n",
    "Once the model is fit, it can be evaluated on the test dataset.\n",
    "\n",
    "This can be achieved by using the DataLoader for the test dataset and collecting the predictions for the test set, then comparing the predictions to the expected values of the test set and calculating a performance metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # evaluate the model on the test set\n",
    "    yhat = model(inputs)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions\n",
    "\n",
    "A fit model can be used to make a prediction on new data.\n",
    "\n",
    "For example, you might have a single image or a single row of data and want to make a prediction.\n",
    "\n",
    "This requires that you wrap the data in a PyTorch Tensor data structure.\n",
    "\n",
    "A Tensor is just the PyTorch version of a NumPy array for holding data. It also allows you to perform the automatic differentiation tasks in the model graph, like calling backward() when training the model.\n",
    "\n",
    "The prediction too will be a Tensor, although you can retrieve the NumPy array by detaching the Tensor from the automatic differentiation graph and calling the NumPy function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# convert row to data\n",
    "row = Variable(Tensor([row]).float())\n",
    "# make prediction\n",
    "yhat = model(row)\n",
    "# retrieve numpy array\n",
    "yhat = yhat.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
